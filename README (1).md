# ğŸ§  Neural Network from Scratch in NumPy

This project implements a fully functioning feedforward neural network from the ground up â€” **without using any deep learning libraries** like TensorFlow or PyTorch. Just pure NumPy.

It includes:
- Dense layers (fully connected)
- ReLU and Softmax activations
- Categorical Cross-Entropy loss
- Forward pass for multi-class classification using synthetic spiral data

---

## ğŸš€ Features

- âœ… Built from scratch using NumPy
- âœ… Layered, class-based architecture
- âœ… Clean and modular file structure
- âœ… End-to-end forward pass logic
- ğŸš§ Backpropagation planned in future (in a separate repo)

---

## ğŸ“‚ File Structure

```
nn_numpy/
â”‚
â”œâ”€â”€ dense_layers_with_activation.py     # Main script to run the forward pass
â”œâ”€â”€ dot_product.py                      # Manual implementation of dot product with bias
â”œâ”€â”€ layers.py                           # Layer and activation classes
â”œâ”€â”€ softmax.py                          # Softmax activation logic
â”œâ”€â”€ spiral_data.py                      # Generates spiral classification dataset
â”œâ”€â”€ __init__.py                         # Marks the directory as a Python module
â”œâ”€â”€ __pycache__/                        # Compiled Python cache files
â””â”€â”€ README.md                           # Project overview (this file)
```

---

## ğŸ§ª How to Run

1. Clone the repo:
   ```bash
   git clone https://github.com/varunxsensei/neural_network_from_scratch.git
   cd neural_network_from_scratch/nn_numpy
   ```

2. Install dependencies:
   ```bash
   pip install numpy
   ```

3. Run the script:
   ```bash
   python dense_layers_with_activation.py
   ```

Youâ€™ll see the output of the network after the ReLU and Softmax activations, as well as the calculated loss.

---

## ğŸ“š What You'll Learn

- The math behind neural network layers (dot product + bias)
- How ReLU and Softmax functions work
- How to compute Categorical Cross-Entropy loss manually
- How to build and structure neural networks from scratch

---

## ğŸ›  Next Steps

The next part of this project (planned for a new repo):
- ğŸ” Backpropagation logic
- ğŸ¯ Gradient Descent updates
- ğŸ“ˆ Model training loop
- ğŸ“Š Accuracy and performance tracking

---

## ğŸ‘¨â€ğŸ’» Author

Made with â¤ï¸ by [Varun](https://github.com/varunxsensei)  
Aspiring ML & NLP Engineer | Deep Learning Enthusiast

---

## ğŸ“„ License

MIT License â€” feel free to use, modify, and share!