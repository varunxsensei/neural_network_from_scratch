# ðŸ§  Neural Network from Scratch in NumPy

This project implements a fully functioning feedforward neural network from the ground up â€” **without using any deep learning libraries** like TensorFlow or PyTorch. Just pure NumPy.

It includes:
- Dense layers (fully connected)
- ReLU and Softmax activations
- Categorical Cross-Entropy loss
- Forward pass for multi-class classification using synthetic spiral data

---

## ðŸš€ Features

- âœ… Built from scratch using NumPy
- âœ… Layered, class-based architecture
- âœ… Clean and modular file structure
- âœ… End-to-end forward pass logic
- ðŸš§ Backpropagation planned in future (in a separate repo)

---

## ðŸ“‚ File Structure

```
nn_numpy/
â”‚
â”œâ”€â”€ dense_layers_with_activation.py     # Main script to run the forward pass
â”œâ”€â”€ dot_product.py                      # Manual implementation of dot product with bias
â”œâ”€â”€ layers.py                           # Layer and activation classes
â”œâ”€â”€ softmax.py                          # Softmax activation logic
â”œâ”€â”€ spiral_data.py                      # Generates spiral classification dataset
â”œâ”€â”€ __init__.py                         # Marks the directory as a Python module
â”œâ”€â”€ __pycache__/                        # Compiled Python cache files
â””â”€â”€ README.md                           # Project overview (this file)
```
---

## ðŸ§ª How to Run

1. Clone the repo:
   ```bash
   git clone https://github.com/varunxsensei/neural_network_from_scratch.git
   cd neural_network_from_scratch/nn_numpy
   ```

2. Install dependencies:
   ```bash
   pip install numpy
   ```

3. Run the script:
   ```bash
   python dense_layers_with_activation.py
   ```

Youâ€™ll see the output of the network after the ReLU and Softmax activations, as well as the calculated loss.

---

## ðŸ“š What You'll Learn

- The math behind neural network layers (dot product + bias)
- How ReLU and Softmax functions work
- How to compute Categorical Cross-Entropy loss manually
- How to build and structure neural networks from scratch

---

## ðŸ›  Next Steps

The next part of this project (planned for a new repo):
- Backpropagation logic
- Gradient Descent updates
- Model training loop
- Accuracy and performance tracking

---

## ðŸ“„ License

MIT License â€” feel free to use, modify, and share!
